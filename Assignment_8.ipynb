{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-free (RL) Prediction With Monte Carlo and Temporal Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC methods learn directly from episodes of experience\n",
    "- MC is model-free:  no knowledge of MDP transitions / rewards\n",
    "- MC learns fromcompleteepisodes:  no bootstrapping\n",
    "- MC uses the simplest possible idea:  value = mean return\n",
    "- Caveat: can only apply MC toepisodicMDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 First Visit Monte Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\textbf{first}$ time-step $t$ that state $s$ is visited in an episode,\n",
    "- Increment counter $N(s)\\leftarrow N(s) + 1$\n",
    "- Increment total return $S(s)\\leftarrow S(s) + G_t$ \n",
    "- Value is estimated by mean return $V(s) =S(s)/N(s)$\n",
    "- By law of large numbers,$V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2 Every Visit Monte Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Every}$ time-step $t$ that state $s$ is visited in an episode,\n",
    "- Increment counter $N(s)\\leftarrow N(s) + 1$\n",
    "- Increment total return $S(s)\\leftarrow S(s) + G_t$ \n",
    "- Value is estimated by mean return $V(s) =S(s)/N(s)$\n",
    "- Again,$V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Temporal Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD methods learn directly from episodes of experience\n",
    "- TD is model-free:  no knowledge of MDP transitions / rewards\n",
    "- TD learns from incomplete episodes, by bootstrapping\n",
    "- TD updates a guess towards a guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Applications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.1 Write code for the interface for tabular RL algorithms. The core of this interface should be a mapping from a (state, action) pair to a sampling of the (next state, reward) pair. It is important that this interface doesn't present the state-transition probability model or the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator_example(state, action):\n",
    "    next_state=np.random.uniform(0, 20, size=None)\n",
    "    action=state + action\n",
    "    return next_state, action     \n",
    "\n",
    "def get_mdp_rep_for_rl_tabular(data):\n",
    "    output=[]\n",
    "    for i in data:\n",
    "        next_state, action=simulator_example(i[0],i[1])\n",
    "        output.append([next_state,action])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[17.161130164650224, 5],\n",
       " [0.14653891800612273, 5],\n",
       " [2.0126154840764654, 14],\n",
       " [4.29203779722436, 6],\n",
       " [18.415533264095327, 5]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[[0,5],[2,3],[12,2],[6,0],[4,1]]\n",
    "get_mdp_rep_for_rl_tabular(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2 Implement any tabular Monte-Carlo algorithm for Value Function prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
