{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Week 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MP (Markov Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markov Process (MP) satisfies the Markov property:\n",
    "$$\n",
    "P(S_{t+1}|S_{t}, S_{t-1}, S_{t-2}\\dots) = P(S_{t+1}|S_{t})\n",
    "$$\n",
    "\n",
    "So, in MDP process, we can represent the transformission probability as: \n",
    "$$\n",
    "P_{ss'} = P(S_{t+1} = s' |S_{t} = s)\n",
    "$$\n",
    "\n",
    "If we consider discrete time model, we call it a Markov Chain where probability varies with time:\n",
    "$$\n",
    "P^t_{ss'} = P^t(S_{t+1} = s' |S_{t} = s)\n",
    "$$\n",
    "\n",
    "In a lot of cases we only consider a time-homogeneous(or time-independent) transition probablity.\n",
    "\n",
    "Now let's consider the student problem shown in David Silver's slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find transition matrix :\n",
      " [[0.  0.5 0.  0.  0.  0.5 0. ]\n",
      " [0.  0.  0.8 0.  0.  0.  0.2]\n",
      " [0.  0.  0.  0.6 0.4 0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  1. ]\n",
      " [0.2 0.4 0.4 0.  0.  0.  0. ]\n",
      " [0.1 0.  0.  0.  0.  0.9 0. ]\n",
      " [0.  0.  0.  0.  0.  0.  1. ]] \n",
      "\n",
      "Find stationary distribution:\n",
      " {'C1': 0.0, 'C2': 0.0, 'C3': 0.0, 'Pass': 0.0, 'Pub': 0.0, 'Facebook': 0.0, 'Sleep': 1.0} \n",
      "\n",
      "Find all states:\n",
      " ['C1', 'C2', 'C3', 'Pass', 'Pub', 'Facebook', 'Sleep'] \n",
      "\n",
      "Find sink state:\n",
      " {'Sleep'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eig\n",
    "from typing import Mapping,Sequence,Set\n",
    "import sys\n",
    "sys.path.append('Code/Processes/Markov')\n",
    "sys.path.append('Code/Utils')\n",
    "from Markov_Functions import verify_mp\n",
    "from Generic_TypeVars import S\n",
    "from Standard_TypeVars import SSf\n",
    "\n",
    "class MP:\n",
    "    # The Mapping type indicates a dict[state, dict[state, number]]\n",
    "    # self.transition_graph is a dictionary\n",
    "    def __init__(self, graph: SSf):\n",
    "        if verify_mp(graph):\n",
    "            self.state: Sequence[S] = list(graph.keys())\n",
    "            self.transition_graph: Mapping[S, Mapping[S, float]] = graph\n",
    "            self.transition_matrix: np.array = self.find_transition_matrix()\n",
    "        else:\n",
    "            raise ValueError\n",
    "    \n",
    "    def find_transition_matrix(self)->np.array:\n",
    "        '''Transition Matrix'''\n",
    "        num: int = len(self.state)\n",
    "        transition_matrix = np.zeros((num,num))\n",
    "        for i in range(num):\n",
    "            s=self.state[i]\n",
    "            for j in range(num):\n",
    "                s_prime=self.state[j]\n",
    "                if s_prime in self.transition_graph[s].keys():\n",
    "                    transition_matrix[i,j]=self.transition_graph[s][s_prime]\n",
    "        return transition_matrix    \n",
    "   \n",
    "    def find_transition_probability(self,cur_state:S,next_state:S)->float:\n",
    "        '''Get specific transition Probabilities between two states'''\n",
    "        if cur_state in self.state and next_state in self.state:\n",
    "            if next_state in self.transition_graph[cur_state].keys():\n",
    "                \n",
    "                return self.transition_graph[cur_state][next_state]\n",
    "            else:\n",
    "                return 0.0\n",
    "        else: \n",
    "            raise ValueError\n",
    "            \n",
    "    def find_stationary_distribution(self) -> Mapping[S, Mapping[S, float]]:\n",
    "        '''Stationary Probabilities'''\n",
    "        # We use the eigen-value approach\n",
    "        eig_val, eig_vec_right = eig(self.transition_matrix.T)\n",
    "        eig_vec_left = eig_vec_right[:,np.isclose(eig_val, 1)]\n",
    "        eig_vec = eig_vec_left[:,0]\n",
    "        stationary = eig_vec/eig_vec.sum()\n",
    "        stationary = stationary.real\n",
    "        return {s: stationary[i] for i, s in enumerate(self.state)}\n",
    "    \n",
    "    def check_stationary_distribution(self) -> np.array:\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def find_transition_graph(self) -> Mapping[S, Mapping[S, float]]:\n",
    "        '''Current Graph'''\n",
    "        return self.transition_graph\n",
    "    \n",
    "    def find_all_states(self) -> Sequence[S]:\n",
    "        '''Current State'''\n",
    "        return self.state    \n",
    "    \n",
    "    def find_sink_states(self) -> Set[S]:\n",
    "        return {k for k, v in self.transition_graph.items()\n",
    "                if len(v) == 1 and k in v.keys()}\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    student = {'C1':{'C2': 0.5, 'Facebook': 0.5},\n",
    "               'C2':{'C3': 0.8, 'Sleep': 0.2},\n",
    "               'C3':{'Pass': 0.6, 'Pub': 0.4},\n",
    "               'Pass':{'Sleep': 1.0},\n",
    "               'Pub':{'C1': 0.2, 'C2': 0.4, 'C3': 0.4},\n",
    "               'Facebook':{'C1': 0.1, 'Facebook': 0.9},\n",
    "               'Sleep':{'Sleep': 1.0}}\n",
    "    \n",
    "    test_mp=MP(student)\n",
    "    print('Find transition matrix :\\n',test_mp.find_transition_matrix(),'\\n')\n",
    "    print('Find stationary distribution:\\n',test_mp.find_stationary_distribution(),'\\n')\n",
    "    print('Find all states:\\n',test_mp.find_all_states(),'\\n')\n",
    "    print('Find sink state:\\n',test_mp.find_sink_states(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. MRP (Markov Reward Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the MP property, the  Markov Reward Proces (MRP) takes the Reward $R_t $, as well as the Discount Factor $\\gamma \\in [0,1] $ into account. We are going to model the transmission of  State $S$. We still need State $S$ and Transition Probability $P$, and also the reward function. However, there are several definitions about reward function:\n",
    "\n",
    "$$\\text{ David Silver} : R_{s}=E[R_{t+1} \\mid S_t=s]$$\n",
    "\n",
    "which is an expected value of $s^{'}$, or mathematically\n",
    "\n",
    "$$ R_{s} = \\sum_{s' \\in S}^{}{P_{ss'}R_{ss'}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also introduce a return $G_t$ which can be calculated as:\n",
    "\n",
    "$$G_t=R_{t+1}+\\gamma R_{t+2}+...=\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}$$\n",
    "\n",
    "The discount factor should be chosen based on certain situations, rather than need of convergence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on definitions and functions of MRP, we can come up with  Value Function $(V(s))$, which is a function of state $S$: \n",
    "$$v(s)=E[G_t \\mid S_t=s]$$ \n",
    "\n",
    "$v(s)$ has this explanation: the expected return of state s will be $v(s)$. Expand the expected value function: $$\\begin{split} v(s) & =  E[G_t \\mid S_t=s] \\\\ & =  E[R_{t+1}+ \\gamma R_{t+2} + \\gamma^2 R_{t+3}+... \\mid S_t=s] \\\\ & =  E[R_{t+1}+ \\gamma G_{t+1} \\mid S_t=s] \\\\ & =  E[R_{t+1}+ \\gamma v(S_{t+1}) \\mid S_t=s] \\\\ & =   R_{s} + \\gamma \\sum_{s^{'} \\in  S} P_{ss^{'}}v(s^{'}) \\end{split}$$ So we can see, the value of each state is equal to the conditional expected value of immediate reward $R_{s}$ , plus the dicounted value of successor state $\\gamma v(S_{t+1})$, based on that given state $S_t=s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, given an MRP, all states have their unique value functions. Obviously, it depends on the transition probability and reward in each state.\n",
    "\n",
    "Then we can find out such equation, called Bellman Equation:\n",
    "$$v(s)= R_{s} + \\gamma \\sum_{s^{'} \\in  S}  P_{ss^{'}}v(s^{'})$$\n",
    "\n",
    "Further more, the matrix form can be written as:\n",
    "$$v=(I- \\gamma  P)^{-1}  R$$\n",
    "\n",
    "Let's still consider the student problem shown in David Silver's slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get value function vector for gamma = 1:\n",
      " [-12.54318511   1.45679124   4.32098948  10.           0.80247529\n",
      " -22.54316356   0.        ] \n",
      "\n",
      "Find reward vector:\n",
      " [-2. -2. -2. 10.  1. -1.  0.]\n"
     ]
    }
   ],
   "source": [
    "### MRP add two more properties on MP\n",
    "#in this section, the default reward function based on david silver definition\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('Code/Utils')\n",
    "sys.path.append('Code/Processes/Markov')\n",
    "from MP import MP\n",
    "from typing import Mapping\n",
    "from Generic_TypeVars import S\n",
    "from Standard_TypeVars import STSff\n",
    "from General_Utils import zip_dict_of_tuple\n",
    "\n",
    "class MRP(MP):\n",
    "    \n",
    "    def __init__(self, mrp_graph:STSff, gamma:float):\n",
    "        # Super Class from MP\n",
    "        mp_graph, reward_graph = zip_dict_of_tuple(mrp_graph)\n",
    "        super().__init__(mp_graph)                                        \n",
    "        self.reward_graph: Mapping[S, float] = reward_graph\n",
    "        # Gamma is the discount factor\n",
    "        self.gamma: float = gamma\n",
    "        self.reward_vector=self.find_reward_vector()\n",
    "        \n",
    "    def find_reward(self) -> Mapping[S, float]:\n",
    "        return self.reward_graph\n",
    "\n",
    "    def find_reward_vector(self) -> np.array:\n",
    "        return np.array([self.reward_graph[i] for i in self.state])\n",
    "    \n",
    "    def find_value_function_vector(self) -> np.array:\n",
    "        # Bellman Equation in Matrix Form\n",
    "        # Blows up when gamma = 1\n",
    "        return np.linalg.inv(np.eye(len(self.state)) - self.gamma * self.transition_matrix\n",
    "        ).dot(self.find_reward_vector())\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    student = {'C1':({'C2': 0.5, 'Facebook': 0.5}, -2.0),\n",
    "               'C2':({'C3': 0.8, 'Sleep': 0.2}, -2.0),\n",
    "               'C3':({'Pass': 0.6, 'Pub': 0.4}, -2.0),\n",
    "               'Pass':({'Sleep': 1.0}, 10.0),\n",
    "               'Pub':({'C1': 0.2, 'C2': 0.4, 'C3': 0.4}, 1.0),\n",
    "               'Facebook':({'C1': 0.1, 'Facebook': 0.9}, -1.0),\n",
    "               'Sleep':({'Sleep': 1.0}, 0.0)}\n",
    "    gamma = 0.9999999\n",
    "    test_mrp = MRP(student, gamma)\n",
    "    print('Get value function vector for gamma = 1:\\n',test_mrp.find_value_function_vector(),'\\n')\n",
    "    print('Find reward vector:\\n', test_mrp.find_reward_vector())   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
