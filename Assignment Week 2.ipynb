{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MDP (Markov Decision Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov Decision Process (MDP) is a Markov reward process with decisions. \n",
    "It is an environment in which all states are Markov.In this case, we can make active decisions to arrive different states, rather than being manipulated passively.\n",
    "\n",
    "As an expansion of MRP, the MDP has 5 components: $S,P,R,A,\\gamma $:\n",
    "\n",
    "- $S$ : A finite set of states which satisfy Markov property.\n",
    "- $A$ : A finite set of actions.\n",
    "- $P$ : A corresponding state transition probability matrix: (now it's a 3-dimension matrix) \n",
    "$$ P_{ss^{'}}^a=P(S_{t+1}=s^{'} \\mid S_t=s, A_t=a)$$\n",
    "- $R$ : A reward function:\n",
    "$$ R_{s}^a=E[R_{t+1} \\mid S_t=s, A_t=a]$$\n",
    "- $\\gamma$: A discount factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A $policy \\ \\pi$ is a distribution over actions given states: $$\\pi(a \\mid s)=P[A_t=a \\mid S_t=s]$$\n",
    "\n",
    "- A policy fully defines the behaviour of an agent.\n",
    "- MDP policies depend on the current state (not the history).\n",
    "- Given an MDP $\\{S, A ,P ,R, \\gamma \\}$ and a policy $\\pi$.\n",
    "- The state sequence, $S_1, S_2, ...$ is a MP $ \\{S, P\\}$.\n",
    "- The state and reward sequance, $S_1, R_2, S_2, ...$ is an MRP $\\{s, P^\\pi ,R^\\pi, \\gamma\\}$\n",
    "    where, if given a specific policy, $$\\mathcal P_{s,s^{'}}^\\pi=\\sum_{a \\in \\mathcal A} \\pi(a \\mid s) \\mathcal P_{ss^{'}}^a$$ $$\\mathcal R_s^\\pi=\\sum_{a \\in \\mathcal A} \\pi(a \\mid s) \\mathcal R_s^a$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Value Function for MDP (Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 definitions for the value function of MDP based on state or action.\n",
    "The state-value function $v_\\pi(s)$ of an MDP is the expected return starting from state $s$ and then following policy $\\pi$: $$v_\\pi(s)=E_\\pi[G_t \\mid S_t=s]$$ \n",
    "\n",
    "The action-value function $q_\\pi(s,a)$ of an MDP is the expected return starting from state $s$, taking action $a$ and then following policy $\\pi$: $$q_\\pi(s,a)=E_\\pi[G_t \\mid S_t=s, A_t=a]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Bellman Equation for MDP (Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar analysis can be taken in bellman equation. Given a fixed policy, weturn MDP into MRP, and then just use the definition before. I prefer calling this prediction as stated in the caption because we are already given the policy in contrast to the optimization part we will discuss later.\n",
    "\n",
    "\n",
    "\n",
    "- The general expressions are: $$v_\\pi(s)=E_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t=s]$$ \n",
    "\n",
    "$$q_\\pi(s,a)=E_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t=s, A_t=a]$$\n",
    "\n",
    "- Consider one-step expectation for $v_\\pi(s)$ and $q_\\pi(s,a)$: \n",
    "\n",
    "A state node extending two action nodes gives:\n",
    "$$v_\\pi(s)=\\sum_{a \\in \\mathcal A} \\pi(a \\mid s) q_\\pi(s,a)$$ While an action node extending to two state nodes gives\n",
    "$$q_\\pi(s,a)=\\mathcal R_s^a + \\gamma \\sum_{s^{'} \\in \\mathcal S} \\mathcal P_{ss^{'}}^a v_\\pi(s^{'})$$\n",
    "\n",
    "- We can come up with iterative expressions for $v_\\pi(s)$ and $q_\\pi(s,a)$ by joining the 2 equations above: $$v_\\pi(s)=\\sum_{a \\in \\mathcal A} \\pi(a \\mid s) \\Big (\\mathcal R_s^a + \\gamma \\sum_{s^{'} \\in \\mathcal S} \\mathcal P_{ss^{'}}^a v_\\pi(s^{'}) \\Big)$$ $$q_\\pi(s,a)=\\mathcal R_s^a + \\gamma \\sum_{s^{'} \\in \\mathcal S} \\mathcal P_{ss^{'}}^a \\sum_{a^{'} \\in \\mathcal A} \\pi(a^{'} \\mid s^{'}) q_\\pi(s^{'},a^{'})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Optimality Value Function for MDP (Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal state-value function  $v_*(s)$ is the maximum state-value function over all policies: $$v_*(s) = \\max _\\pi v_\\pi(s)$$ The optimal action-value function $ q_*(s,a)$ is the maximum action-value function over all policies: $$q_*(s,a) = \\max _\\pi q_\\pi(s,a)$$\n",
    "\n",
    "- The optimal value function specifies the best possible performance in the MDP.\n",
    "- An MDP is \"solved\" when we know the optimal value function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Optimal Policy for MDP (Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the inequality relation over policies as: $\\pi \\geq \\pi^{'} $ if $ v_\\pi(s) \\geq v_{\\pi^{'}}(s), \\forall s$.\n",
    "Theorem: For any MDP,\n",
    "\n",
    "- There exists an optimal policy $\\pi_*$ that is better than or equal to all other policies, $\\pi_* \\geq  \\pi, \\forall \\pi$.\n",
    "- All optimal policies achieve the optimal state-value function, $v_{\\pi_*(s)}=v_*(s)$.\n",
    "- All optimal policies achieve the optimal action-value function, $q_{\\pi_*(s,a)}=q_*(s,a)$.\n",
    "\n",
    "An optimal policy can be found by maximising over $q_*(s,a)$, $$\\pi_*(a \\mid s)= \\begin{cases} 1& \\text{if $a=\\mathop{\\arg\\max}_{a \\in \\mathcal A}q_*(s,a)$}\\\\ 0& \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "- There is always a deterministic optimal policy for any MDP.\n",
    "- If we know $q_*(s,a)$, we immediately have the optimal policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Bellman Optimality Equation for MDP (Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider the optimal policy case in the one-step Bellman equation, we have: $$v_*(s)=\\max_a q_*(s,a)$$ $$q_*(s,a)=\\mathcal R_s^a + \\gamma \\sum_{s^{'} \\in \\mathcal S} \\mathcal P_{ss^{'}}^a v_*(s^{'})$$\n",
    "\n",
    "Again, combine these 2 equations, we have iterative expressions for $v_*(s)$ and $q_*(s,a)$ : $$v_*(s)=\\max_a \\Big (\\mathcal R_s^a + \\gamma \\sum_{s^{'} \\in \\mathcal S} \\mathcal P_{ss^{'}}^a v_*(s^{'}) \\Big)$$ $$q_*(s,a)=\\mathcal R_s^a + \\gamma \\sum_{s^{'} \\in \\mathcal S} \\mathcal P_{ss^{'}}^a \\max_{a^{'}} q_*(s^{'},a^{'})$$\n",
    "\n",
    "Bellman optimality equation is non-linear due to the $\\texttt{max}$ function in the equation. So there is no closed form solution in general. We can use iterative methods to solve it, which will be explored in the next assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Relation between  MP, MRP, and MDP Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These concepts are built step by step. We build MRP by introducing rewards based on MP, and build MDP by introducing decisions based on MRP.\n",
    "\n",
    "So, to summarize:\n",
    "\n",
    "- Given an MDP $ M = \\{S, A ,P ,R, \\gamma\\}$ and a policy $\\pi$.\n",
    "- The state sequence, $S_1, S_2, ...$ is an MP $ \\{S, P^\\pi\\}$, which is the foundation of all MP,MRP and MDP.\n",
    "- The state and reward sequance, $S_1, R_2, S_2, ...$ is an MRP $\\mathcal \\{S, P^\\pi ,R^\\pi,\\gamma\\}$. Which means given a policy, we treat the poicy as \"environment', so now we are back to MRP that following the environment \"passively\"\n",
    "- where, $$\\mathcal P_{s,s^{'}}^\\pi=\\sum_{a \\in \\mathcal A} \\pi(a \\mid s) \\mathcal P_{ss^{'}}^a$$ $$\\mathcal R_s^\\pi=\\sum_{a \\in \\mathcal A} \\pi(a \\mid s) \\mathcal R_s^a$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
