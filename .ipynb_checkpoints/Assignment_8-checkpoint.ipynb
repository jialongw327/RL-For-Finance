{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-free (RL) Prediction With Monte Carlo and Temporal Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC methods learn directly from episodes of experience\n",
    "- MC is model-free:  no knowledge of MDP transitions / rewards\n",
    "- MC learns fromcompleteepisodes:  no bootstrapping\n",
    "- MC uses the simplest possible idea:  value = mean return\n",
    "- Caveat: can only apply MC toepisodicMDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 First Visit Monte Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\textbf{first}$ time-step $t$ that state $s$ is visited in an episode,\n",
    "- Increment counter $N(s)\\leftarrow N(s) + 1$\n",
    "- Increment total return $S(s)\\leftarrow S(s) + G_t$ \n",
    "- Value is estimated by mean return $V(s) =S(s)/N(s)$\n",
    "- By law of large numbers,$V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2 Every Visit Monte Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Every}$ time-step $t$ that state $s$ is visited in an episode,\n",
    "- Increment counter $N(s)\\leftarrow N(s) + 1$\n",
    "- Increment total return $S(s)\\leftarrow S(s) + G_t$ \n",
    "- Value is estimated by mean return $V(s) =S(s)/N(s)$\n",
    "- Again,$V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Temporal Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD methods learn directly from episodes of experience\n",
    "- TD is model-free:  no knowledge of MDP transitions / rewards\n",
    "- TD learns from incomplete episodes, by bootstrapping\n",
    "- TD updates a guess towards a guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Applications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.1 Write code for the interface for tabular RL algorithms. The core of this interface should be a mapping from a (state, action) pair to a sampling of the (next state, reward) pair. It is important that this interface doesn't present the state-transition probability model or the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(state, action):\n",
    "    next_state=np.random.uniform(0, 20, size=None)\n",
    "    action=state + action\n",
    "    return next_state, action     \n",
    "\n",
    "def get_mdp_rep_for_rl_tabular(data):\n",
    "    output=[]\n",
    "    for i in data:\n",
    "        next_state, action=simulator(i[0],i[1])\n",
    "        output.append([next_state,action])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.822124337799695, 5],\n",
       " [14.940305864168437, 5],\n",
       " [3.6953362781060184, 14],\n",
       " [7.149493978261985, 6],\n",
       " [4.543915349097594, 5]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[[0,5],[2,3],[12,2],[6,0],[4,1]]\n",
    "get_mdp_rep_for_rl_tabular(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2 Implement any tabular Monte-Carlo algorithm for Value Function prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC(sample_size, mc_path, states, visit=\"first\"):\n",
    "    V, count, _= build_dict_MC(sample_size, states)\n",
    "    for episode in mc_path:\n",
    "        _, _,first_visit = build_dict_MC(sample_size, states)\n",
    "        for i in episode:\n",
    "            s=i[0]\n",
    "            G=i[1]\n",
    "            if  visit==\"first\":\n",
    "                if first_visit[s]==0:\n",
    "                    count[s] += 1\n",
    "                    c = count[s]\n",
    "                    V[s] = (V[s] * (c - 1) + G) / c\n",
    "                    first_visit[s]=1\n",
    "            if  visit==\"every\":\n",
    "                count[s] += 1\n",
    "                c = count[s]\n",
    "                V[s] = (V[s] * (c - 1) + G) / c\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_MC(size, states):\n",
    "    value_dict={}\n",
    "    count_dict={}\n",
    "    first_visit_dict = {}\n",
    "    for i in range(size):\n",
    "        value_dict[i]=0\n",
    "        count_dict[i]=0\n",
    "    for s in states:\n",
    "        first_visit_dict[s] = 0\n",
    "    return value_dict, count_dict, first_visit_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_path=[\n",
    "    [[0,5],[1,3],[3,2],[0,2],[5,2]],\n",
    "    [[0,3],[2,3],[4,3]],\n",
    "    [[1,6],[2,4],[4,3],[1,2],[5,2]],\n",
    "    [[1,6],[3,4],[4,3],[3,2],[0,2]],\n",
    "    [[1,6],[4,4],[4,3],[2,2],[1,2]],\n",
    "]\n",
    "\n",
    "states = [0, 1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3.3333333333333335, 1: 5.25, 2: 3.0, 3: 3.0, 4: 3.25, 5: 2.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MC(6, mc_path, states, \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3.0, 1: 4.166666666666667, 2: 3.0, 3: 2.6666666666666665, 4: 3.2, 5: 2.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MC(6, mc_path, states, \"every\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.3 Implement tabular 1-step TD algorithm for Value Function prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD(sample_size,TD_path,gamma,alpha=0):\n",
    "    V, count=build_dict_TD(sample_size)\n",
    "    \n",
    "    for episode in TD_path:\n",
    "        for j,i in enumerate(episode):\n",
    "            s=i[0]\n",
    "            r=i[1]\n",
    "            if j<len(episode)-1:\n",
    "                next_s=episode[j+1][0]\n",
    "            else:\n",
    "                next_s=i[0]\n",
    "            count[s] += 1\n",
    "            if alpha==0:\n",
    "                alpha = 1/count[s]\n",
    "            V[s] = V[s] + alpha*(r+gamma*V[next_s]-V[s])\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_TD(size):\n",
    "    value_dict={}\n",
    "    count_dict={}\n",
    "    for i in range(size):\n",
    "        value_dict[i]=0\n",
    "        count_dict[i]=0\n",
    "    return value_dict, count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TD_path contains the data of episodes. Each episode there are (state, r) pairs.\n",
    "TD_path=[\n",
    "    [[0,5],[1,3],[3,2],[0,2],[5,2]],\n",
    "    [[0,3],[2,3],[4,3]],\n",
    "    [[1,6],[2,4],[4,3],[1,2],[5,2]],\n",
    "    [[1,6],[3,4],[4,3],[3,2],[0,2]],\n",
    "    [[1,6],[4,4],[4,3],[2,2],[1,2]],\n",
    "    [[1,6],[3,4],[4,3],[3,2],[0,2]],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 7.0, 1: 11.0, 2: 27.0, 3: 7.0, 4: 17.0, 5: 4.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TD(6,TD_path,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 6.08,\n",
       " 1: 13.432576000000001,\n",
       " 2: 16.391424,\n",
       " 3: 7.6874752,\n",
       " 4: 14.4373248,\n",
       " 5: 3.2}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TD(6,TD_path,1,0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
