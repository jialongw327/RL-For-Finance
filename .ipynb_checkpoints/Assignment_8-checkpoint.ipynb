{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-free (RL) Prediction With Monte Carlo and Temporal Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC methods learn directly from episodes of experience\n",
    "- MC is model-free:  no knowledge of MDP transitions / rewards\n",
    "- MC learns fromcompleteepisodes:  no bootstrapping\n",
    "- MC uses the simplest possible idea:  value = mean return\n",
    "- Caveat: can only apply MC toepisodicMDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.1 First Visit Monte Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\textbf{first}$ time-step $t$ that state $s$ is visited in an episode,\n",
    "- Increment counter $N(s)\\leftarrow N(s) + 1$\n",
    "- Increment total return $S(s)\\leftarrow S(s) + G_t$ \n",
    "- Value is estimated by mean return $V(s) =S(s)/N(s)$\n",
    "- By law of large numbers,$V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.2 Every Visit Monte Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Every}$ time-step $t$ that state $s$ is visited in an episode,\n",
    "- Increment counter $N(s)\\leftarrow N(s) + 1$\n",
    "- Increment total return $S(s)\\leftarrow S(s) + G_t$ \n",
    "- Value is estimated by mean return $V(s) =S(s)/N(s)$\n",
    "- Again,$V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Temporal Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD methods learn directly from episodes of experience\n",
    "- TD is model-free:  no knowledge of MDP transitions / rewards\n",
    "- TD learns from incomplete episodes, by bootstrapping\n",
    "- TD updates a guess towards a guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Applications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.1 Write code for the interface for tabular RL algorithms. The core of this interface should be a mapping from a (state, action) pair to a sampling of the (next state, reward) pair. It is important that this interface doesn't present the state-transition probability model or the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
