{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming (DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Problem: evaluate a given policy $\\pi$\n",
    "- Solution: iterative application of Bellman expectation backup\n",
    "\n",
    "$$v_{k+1}= R_\\pi + \\gamma  P_\\pi v_k$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(self, pol:Policy) -> np.array:\n",
    "    '''Iterative way to find the value functions given a specific policy'''\n",
    "    mrp = self.find_mrp(pol)\n",
    "    v0 = np.zeros(len(self.states))\n",
    "    converge = False\n",
    "    while not converge:\n",
    "        v1 = mrp.reward_vector + self.gamma*mrp.transition_matrix.dot(v0)\n",
    "        converge = is_approx_eq(np.linalg.norm(v1), np.linalg.norm(v0))\n",
    "        v0 = v1\n",
    "    return v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy evaluation: estimate $v_\\pi$ by some policy evaluation algorithm (e.g. iterative)\n",
    "- Policy improvement: generate $\\pi' \\geq \\pi$ by some policy improvement algorithm (e.g. greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_improved_policy(self, pol: Policy) -> DetPolicy:\n",
    "    '''Find the policy that maximizes the act-value function in one iteration (greedy)'''\n",
    "    q_dict = self.find_act_value_func_dict(pol)\n",
    "    return DetPolicy({s: max(v.items(), key=itemgetter(1))[0]\n",
    "                          for s, v in q_dict.items()})\n",
    "            \n",
    "def policy_iteration(self, tol=1e-4) -> DetPolicy:\n",
    "    ''' Find the optimal policy using policy iteration '''\n",
    "    pol = Policy({s: {a: 1. / len(v) for a in v} for s, v in\n",
    "                      self.state_action_dict.items()})\n",
    "    vf = self.find_value_func_dict(pol)\n",
    "    epsilon = tol * 1e4\n",
    "    while epsilon >= tol:\n",
    "        pol = self.find_improved_policy(pol)\n",
    "        new_vf = self.find_value_func_dict(pol)\n",
    "        epsilon = max(abs(new_vf[s] - v) for s, v in vf.items())\n",
    "        vf = new_vf\n",
    "    return pol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Problem: find optimal policy $\\pi$.\n",
    "- Only initialize $v_0$, no policy needed. \n",
    "- Solution: iterative application of Bellman optimality backup. Only extracts the best policy at last. \n",
    "\n",
    "$$v_{k+1}=\\max_{a \\in  A}  R^a + \\gamma  P^av_k$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_1(self, tol = 1e-4) -> DetPolicy:\n",
    "    # Initialize value-function\n",
    "    vf = np.zeros(len(self.all_states))\n",
    "    transition_graph = self.transitions\n",
    "    # action_graph = [{s:[k for k, i in v.items()] for s, v in reward_graph.items()}]\n",
    "    action_sequence_list = [[k for k, i in v.items()] for s, v in self.rewards.items()]\n",
    "    # Permute through all probable paths of actions\n",
    "    action_permutations = list(itertools.product(*action_sequence_list))\n",
    "    reward_list, prob_list = [v for s, v in self.rewards.items()], \\\n",
    "    [v for s, v in self.transitions.items()]\n",
    "    # Initialize epsilon, the residual\n",
    "    epsilon = 1\n",
    "    while epsilon > tol:\n",
    "        value_function_matrix = np.zeros((len(action_permutations),len(self.all_states)))\n",
    "        # Find transition matrix and reward vector for each permutation\n",
    "        for i in range(len(action_permutations)):\n",
    "            current_rewards = []\n",
    "            current_transition_graph = {}\n",
    "            for j in range(len(self.all_states)):\n",
    "                current_rewards.append(reward_list[j][action_permutations[i][j]])\n",
    "                current_transition_graph[list(transition_graph.keys())[j]]\\\n",
    "                = prob_list[j][action_permutations[i][j]]\n",
    "\n",
    "            mp_obj = MP(current_transition_graph)\n",
    "            current_transition_matrix = mp_obj.transition_matrix\n",
    "            value_function_matrix[i,:] = current_rewards + self.gamma * current_transition_matrix\\\n",
    "                                                     .dot(vf)     \n",
    "        # Sort out the best value_function \n",
    "        k = len(self.all_states)-1\n",
    "        value_function_matrix = value_function_matrix[value_function_matrix[:,k].argsort()]\n",
    "        while k != 0:\n",
    "            k = k-1\n",
    "            pol_ind = value_function_matrix[:,k].argsort(kind='mergesort')\n",
    "            value_function_matrix = value_function_matrix[pol_ind]   \n",
    "        # Update the residual\n",
    "        epsilon = max(np.absolute(value_function_matrix[-1,:]-vf))\n",
    "        vf = value_function_matrix[-1,:]\n",
    "        # Extract the optimal policy     \n",
    "    pol = {self.all_states[i]: action_permutations[pol_ind[0]][i] \\\n",
    "               for i in range(len(self.all_states))} \n",
    "    print(pol)\n",
    "    return DetPolicy(pol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
